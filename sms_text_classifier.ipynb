{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7sSELBZVNzD3qM23OC7fu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullahwarraichh/Machine-Learning-Projects/blob/main/sms_text_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PdG2UZ2nE1bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwFnJsE6vjf8",
        "outputId": "bedd68ac-7d3a-49fe-8434-8f6de2c3a6db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-03 22:07:06--  https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
            "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 104.26.2.33, 172.67.70.149, 104.26.3.33, ...\n",
            "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|104.26.2.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 358233 (350K) [text/tab-separated-values]\n",
            "Saving to: ‘train_data.tsv’\n",
            "\n",
            "train_data.tsv      100%[===================>] 349.84K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-11-03 22:07:06 (7.07 MB/s) - ‘train_data.tsv’ saved [358233/358233]\n",
            "\n",
            "--2024-11-03 22:07:06--  https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
            "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 104.26.2.33, 172.67.70.149, 104.26.3.33, ...\n",
            "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|104.26.2.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118774 (116K) [text/tab-separated-values]\n",
            "Saving to: ‘test_data.tsv’\n",
            "\n",
            "test_data.tsv       100%[===================>] 115.99K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-11-03 22:07:06 (5.08 MB/s) - ‘test_data.tsv’ saved [118774/118774]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "131/131 - 9s - 69ms/step - accuracy: 0.9186 - loss: 0.2314 - val_accuracy: 0.9691 - val_loss: 0.0989\n",
            "Epoch 2/1000\n",
            "131/131 - 4s - 27ms/step - accuracy: 0.9844 - loss: 0.0567 - val_accuracy: 0.9835 - val_loss: 0.0549\n",
            "Epoch 3/1000\n",
            "131/131 - 4s - 30ms/step - accuracy: 0.9916 - loss: 0.0284 - val_accuracy: 0.9856 - val_loss: 0.0452\n",
            "Epoch 4/1000\n",
            "131/131 - 3s - 20ms/step - accuracy: 0.9964 - loss: 0.0145 - val_accuracy: 0.9864 - val_loss: 0.0413\n",
            "Epoch 5/1000\n",
            "131/131 - 2s - 17ms/step - accuracy: 0.9986 - loss: 0.0082 - val_accuracy: 0.9878 - val_loss: 0.0368\n",
            "Epoch 6/1000\n",
            "131/131 - 3s - 26ms/step - accuracy: 0.9998 - loss: 0.0050 - val_accuracy: 0.9864 - val_loss: 0.0420\n",
            "Epoch 7/1000\n",
            "131/131 - 4s - 32ms/step - accuracy: 0.9998 - loss: 0.0039 - val_accuracy: 0.9871 - val_loss: 0.0366\n",
            "Epoch 8/1000\n",
            "131/131 - 4s - 31ms/step - accuracy: 0.9998 - loss: 0.0028 - val_accuracy: 0.9892 - val_loss: 0.0340\n",
            "Epoch 9/1000\n",
            "131/131 - 3s - 25ms/step - accuracy: 0.9998 - loss: 0.0024 - val_accuracy: 0.9878 - val_loss: 0.0351\n",
            "Epoch 10/1000\n",
            "131/131 - 4s - 30ms/step - accuracy: 0.9998 - loss: 0.0018 - val_accuracy: 0.9878 - val_loss: 0.0373\n",
            "Epoch 11/1000\n",
            "131/131 - 4s - 29ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9871 - val_loss: 0.0374\n",
            "Epoch 12/1000\n",
            "131/131 - 5s - 38ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9878 - val_loss: 0.0363\n",
            "Epoch 13/1000\n",
            "131/131 - 3s - 22ms/step - accuracy: 0.9998 - loss: 9.8505e-04 - val_accuracy: 0.9892 - val_loss: 0.0332\n",
            "Epoch 14/1000\n",
            "131/131 - 3s - 21ms/step - accuracy: 1.0000 - loss: 6.1905e-04 - val_accuracy: 0.9864 - val_loss: 0.0396\n",
            "Epoch 15/1000\n",
            "131/131 - 4s - 30ms/step - accuracy: 0.9998 - loss: 5.9980e-04 - val_accuracy: 0.9871 - val_loss: 0.0378\n",
            "Epoch 16/1000\n",
            "131/131 - 3s - 24ms/step - accuracy: 1.0000 - loss: 3.8584e-04 - val_accuracy: 0.9892 - val_loss: 0.0347\n",
            "Epoch 17/1000\n",
            "131/131 - 2s - 17ms/step - accuracy: 1.0000 - loss: 3.3094e-04 - val_accuracy: 0.9878 - val_loss: 0.0377\n",
            "Epoch 18/1000\n",
            "131/131 - 2s - 14ms/step - accuracy: 1.0000 - loss: 2.7481e-04 - val_accuracy: 0.9878 - val_loss: 0.0375\n",
            "Epoch 19/1000\n",
            "131/131 - 3s - 21ms/step - accuracy: 1.0000 - loss: 2.2563e-04 - val_accuracy: 0.9885 - val_loss: 0.0381\n",
            "Epoch 20/1000\n",
            "131/131 - 2s - 15ms/step - accuracy: 1.0000 - loss: 1.9608e-04 - val_accuracy: 0.9892 - val_loss: 0.0375\n",
            "Epoch 21/1000\n",
            "131/131 - 3s - 26ms/step - accuracy: 1.0000 - loss: 1.7258e-04 - val_accuracy: 0.9892 - val_loss: 0.0376\n",
            "Epoch 22/1000\n",
            "131/131 - 4s - 31ms/step - accuracy: 1.0000 - loss: 1.4355e-04 - val_accuracy: 0.9864 - val_loss: 0.0413\n",
            "Epoch 23/1000\n",
            "131/131 - 2s - 15ms/step - accuracy: 1.0000 - loss: 1.3123e-04 - val_accuracy: 0.9885 - val_loss: 0.0396\n",
            "Epoch 24/1000\n",
            "131/131 - 2s - 15ms/step - accuracy: 1.0000 - loss: 1.1544e-04 - val_accuracy: 0.9864 - val_loss: 0.0413\n",
            "Epoch 25/1000\n",
            "131/131 - 3s - 20ms/step - accuracy: 1.0000 - loss: 1.0211e-04 - val_accuracy: 0.9871 - val_loss: 0.0419\n",
            "Epoch 26/1000\n",
            "131/131 - 3s - 22ms/step - accuracy: 1.0000 - loss: 9.3066e-05 - val_accuracy: 0.9871 - val_loss: 0.0419\n",
            "Epoch 27/1000\n",
            "131/131 - 4s - 32ms/step - accuracy: 1.0000 - loss: 8.4568e-05 - val_accuracy: 0.9878 - val_loss: 0.0418\n",
            "Epoch 28/1000\n",
            "131/131 - 2s - 18ms/step - accuracy: 1.0000 - loss: 7.3142e-05 - val_accuracy: 0.9871 - val_loss: 0.0435\n",
            "Epoch 29/1000\n",
            "131/131 - 2s - 19ms/step - accuracy: 1.0000 - loss: 6.8753e-05 - val_accuracy: 0.9871 - val_loss: 0.0428\n",
            "Epoch 30/1000\n",
            "131/131 - 3s - 22ms/step - accuracy: 1.0000 - loss: 6.1022e-05 - val_accuracy: 0.9885 - val_loss: 0.0421\n",
            "Epoch 31/1000\n",
            "131/131 - 3s - 19ms/step - accuracy: 1.0000 - loss: 5.5040e-05 - val_accuracy: 0.9871 - val_loss: 0.0430\n",
            "Epoch 32/1000\n",
            "131/131 - 2s - 15ms/step - accuracy: 1.0000 - loss: 5.0719e-05 - val_accuracy: 0.9892 - val_loss: 0.0417\n",
            "Epoch 33/1000\n",
            "131/131 - 3s - 23ms/step - accuracy: 1.0000 - loss: 4.5487e-05 - val_accuracy: 0.9892 - val_loss: 0.0428\n",
            "Epoch 33: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "You passed the challenge. Well done!\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd  # For handling data in tabular format\n",
        "import tensorflow_datasets as tfds  # For accessing datasets (not used much here)\n",
        "import numpy as np  # For numerical operations\n",
        "import matplotlib.pyplot as plt  # For visualizing data (not used here)\n",
        "from tensorflow.keras.preprocessing.text import one_hot  # For converting words to numbers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For ensuring equal length of input sequences\n",
        "from tensorflow.keras.models import Sequential  # For building a sequential neural network\n",
        "from tensorflow.keras.layers import Flatten, Embedding, Dense  # For building different layers of the neural network\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # To stop training early if performance doesn't improve\n",
        "\n",
        "# Step 1: Downloading and setting file paths for training and testing data\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv -O train_data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv -O test_data.tsv\n",
        "\n",
        "train_data_path = \"train_data.tsv\"\n",
        "test_data_path = \"test_data.tsv\"\n",
        "\n",
        "# Step 2: Reading the data into pandas DataFrames\n",
        "column_names = [\"label\", \"text_message\"]  # Columns: one for the label (ham/spam), one for the actual message\n",
        "train_data = pd.read_csv(train_data_path, sep='\\t', names=column_names)\n",
        "test_data = pd.read_csv(test_data_path, sep='\\t', names=column_names)\n",
        "\n",
        "# Step 3: Extracting messages and labels from the data\n",
        "train_texts = train_data[\"text_message\"].tolist()  # List of all the messages from training data\n",
        "train_labels = np.array([0 if label == \"ham\" else 1 for label in train_data['label']])  # 0 for ham, 1 for spam\n",
        "\n",
        "test_texts = test_data[\"text_message\"].tolist()  # List of all the messages from test data\n",
        "test_labels = np.array([0 if label == \"ham\" else 1 for label in test_data['label']])  # 0 for ham, 1 for spam\n",
        "\n",
        "# Step 4: Creating a dictionary of words (vocabulary) with their counts\n",
        "vocab_dict = {}\n",
        "for message in train_texts:\n",
        "    for word in message.split():  # Split each message into words\n",
        "        if word not in vocab_dict:\n",
        "            vocab_dict[word] = 1  # If word not in vocab, add it with count 1\n",
        "        else:\n",
        "            vocab_dict[word] += 1  # If word already in vocab, increment its count\n",
        "\n",
        "# Step 5: Encoding messages into numbers and padding them to have the same length\n",
        "VOCAB_SIZE = len(vocab_dict)  # Number of unique words (size of the vocabulary)\n",
        "MAX_MESSAGE_LENGTH = max([len(msg.split()) for msg in train_texts])  # Length of the longest message\n",
        "\n",
        "# Convert messages to one-hot encoded integers\n",
        "encoded_train_texts = [one_hot(msg, VOCAB_SIZE) for msg in train_texts]\n",
        "padded_train_texts = pad_sequences(encoded_train_texts, maxlen=MAX_MESSAGE_LENGTH, padding='post')  # Pad the sequences\n",
        "\n",
        "encoded_test_texts = [one_hot(msg, VOCAB_SIZE) for msg in test_texts]\n",
        "padded_test_texts = pad_sequences(encoded_test_texts, maxlen=MAX_MESSAGE_LENGTH, padding='post')  # Pad test sequences\n",
        "\n",
        "# Step 6: Building the machine learning model using Keras\n",
        "model = Sequential()  # Initialize the model\n",
        "\n",
        "# Add an embedding layer that converts words into meaningful vectors\n",
        "model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=100, input_length=MAX_MESSAGE_LENGTH))\n",
        "model.add(Flatten())  # Flatten the output into a single vector\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer with a sigmoid function for binary classification (0 or 1)\n",
        "\n",
        "# Compile the model, specifying optimizer, loss function, and metric to track\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Training the model with early stopping\n",
        "early_stopping_monitor = EarlyStopping(\n",
        "    monitor='val_accuracy',  # Monitor the validation accuracy\n",
        "    patience=25,  # Stop if no improvement after 25 epochs\n",
        "    restore_best_weights=True,  # Restore the weights from the best epoch\n",
        "    verbose=1  # Output training progress\n",
        ")\n",
        "\n",
        "# Fit the model with training data and validate with test data\n",
        "model.fit(\n",
        "    padded_train_texts,  # Training data (messages)\n",
        "    train_labels,  # Training labels (ham/spam)\n",
        "    validation_data=(padded_test_texts, test_labels),  # Validation data (test messages and labels)\n",
        "    epochs=1000,  # Maximum number of epochs to train\n",
        "    callbacks=[early_stopping_monitor],  # Apply early stopping\n",
        "    verbose=2  # Output training details\n",
        ")\n",
        "\n",
        "# Step 8: Function to predict whether a new message is ham or spam\n",
        "def predict_text_message(message):\n",
        "    encoded_msg = one_hot(message, VOCAB_SIZE)  # One-hot encode the message\n",
        "    padded_msg = pad_sequences([encoded_msg], maxlen=MAX_MESSAGE_LENGTH, padding='post')  # Pad the message\n",
        "    prediction = model.predict(padded_msg)[0][0]  # Get the predicted value (0 to 1)\n",
        "    prediction_class = \"ham\" if prediction < 0.5 else \"spam\"  # Classify based on prediction: <0.5 is ham, >=0.5 is spam\n",
        "    return [prediction, prediction_class]  # Return the predicted probability and the class (ham/spam)\n",
        "\n",
        "# Step 9: Function to test the model with a set of sample messages\n",
        "def test_model():\n",
        "    # List of test messages\n",
        "    sample_texts = [\n",
        "        \"how are you doing today\",\n",
        "        \"sale today! to stop texts call 98912460324\",\n",
        "        \"i dont want to go. can we try it a different day? available sat\",\n",
        "        \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "        \"you have won £1000 cash! call to claim your prize.\",\n",
        "        \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "        \"wow, is your arm alright. that happened to me one time too\"\n",
        "    ]\n",
        "\n",
        "    # Correct labels for the sample messages\n",
        "    correct_labels = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "\n",
        "    success = True  # Track if all predictions are correct\n",
        "\n",
        "    # Check each message against its correct label\n",
        "    for text, correct_label in zip(sample_texts, correct_labels):\n",
        "        prediction = predict_text_message(text)\n",
        "        if prediction[1] != correct_label:\n",
        "            success = False  # Mark as false if any prediction is wrong\n",
        "\n",
        "    if success:\n",
        "        print(\"You passed the challenge. Well done!\")\n",
        "    else:\n",
        "        print(\"Some predictions were incorrect. Keep trying.\")\n",
        "\n",
        "# Run the test\n",
        "test_model()\n"
      ]
    }
  ]
}